# 3M3S集群搭建

## 公共配置

### 网络配置

+ /etc/hosts

  ```txt
  192.168.2.190	Master节点负载均衡VIP
  192.168.2.191	k8s-m1
  192.168.2.192 	k8s-m2
  192.168.2.193	k8s-m3
  192.168.2.194	k8s-n1
  192.168.2.195 	k8s-n2
  192.168.2.196 	k8s-n3
  ```

### 安装k8s组件

全部master和node节点都安装。

```shell
# 配置K8S的yum源
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
       http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
# 卸载旧版本
yum remove -y kubelet kubeadm kubectl
# 安装kubelet、kubeadm、kubectl
yum install -y kubelet-1.17.3 kubeadm-1.17.3 kubectl-1.17.3
# 开机启动和重启kubelet
systemctl enable kubelet && systemctl start kubelet
# 注意，如果此时查看kubelet的状态，他会无限重启，等待接收集群命令，和初始化。这个是正常的。
```

## Master节点

### 为３个主节点设置负载均衡

负载均衡服务器监听外部访问6443端口，然后分发到３个主节点的ApiServer(端口6443)。

#### 方案

- **nginx**

- **keepalived + haproxy**

  在三台master主机上部署keepalived:

  ```shell
  #安装
  yum install -y keepalived
  #配置
  cat > /etc/keepalived/keepalived.conf <<EOF 
  ! Configuration File for keepalived
  
  global_defs {
     router_id k8s
  }
  
  vrrp_script check_haproxy {
      script "killall -0 haproxy"
      interval 3
      weight -2
      fall 10
      rise 2
  }
  
  vrrp_instance VI_1 {
      state MASTER 		#其他master节点设置BACKUP
      interface enp0s3	#这里要改为虚拟机的网卡名字
      virtual_router_id 51
      priority 250		#其他master节点小于这个值，如200 150
      advert_int 1
      authentication {
          auth_type PASS
          auth_pass ceb1b3ec013d66163d6ab
      }
      virtual_ipaddress {
          192.168.2.190	#设置同一网段地址
      }
      track_script {
          check_haproxy
      }
  }
  EOF
  #开机启动
  systemctl enable keepalived.service
  systemctl start keepalived.service
  systemctl status keepalived.service
  #查看网卡信息
  ip a s enp0s3
  ```

  在三台master主机上部署haproxy:

  ```shell
  yum install -y haproxy
  
  cat > /etc/haproxy/haproxy.cfg << EOF
  #---------------------------------------------------------------------
  # Global settings
  #---------------------------------------------------------------------
  global
      # to have these messages end up in /var/log/haproxy.log you will
      # need to:
      # 1) configure syslog to accept network log events.  This is done
      #    by adding the '-r' option to the SYSLOGD_OPTIONS in
      #    /etc/sysconfig/syslog
      # 2) configure local2 events to go to the /var/log/haproxy.log
      #   file. A line like the following can be added to
      #   /etc/sysconfig/syslog
      #
      #    local2.*                       /var/log/haproxy.log
      #
      log         127.0.0.1 local2
      
      chroot      /var/lib/haproxy
      pidfile     /var/run/haproxy.pid
      maxconn     4000
      user        haproxy
      group       haproxy
      daemon 
         
      # turn on stats unix socket
      stats socket /var/lib/haproxy/stats
  #---------------------------------------------------------------------
  # common defaults that all the 'listen' and 'backend' sections will
  # use if not designated in their block
  #---------------------------------------------------------------------  
  defaults
      mode                    http
      log                     global
      option                  httplog
      option                  dontlognull
      option http-server-close
      option forwardfor       except 127.0.0.0/8
      option                  redispatch
      retries                 3
      timeout http-request    10s
      timeout queue           1m
      timeout connect         10s
      timeout client          1m
      timeout server          1m
      timeout http-keep-alive 10s
      timeout check           10s
      maxconn                 3000
  #---------------------------------------------------------------------
  # kubernetes apiserver frontend which proxys to the backends
  #--------------------------------------------------------------------- 
  frontend kubernetes-apiserver
      mode                 tcp
      bind                 *:16443		#代理6443端口
      option               tcplog
      default_backend      kubernetes-apiserver    
  #---------------------------------------------------------------------
  # round robin balancing between the various backends
  #---------------------------------------------------------------------
  backend kubernetes-apiserver
      mode        tcp
      balance     roundrobin
      server      m1.k8s.io   192.168.2.191:6443 check
      server      m2.k8s.io   192.168.2.192:6443 check
      server      m3.k8s.io   192.168.2.193:6443 check
  #---------------------------------------------------------------------
  # collection haproxy statistics message
  #---------------------------------------------------------------------
  listen stats
      bind                 *:1080
      stats auth           admin:awesomePassword
      stats refresh        5s
      stats realm          HAProxy\ Statistics
      stats uri            /admin?stats
  EOF
  
  systemctl enable haproxy && systemctl start haproxy &&systemctl status haproxy
  
  yum install net-tools
  netstat -lntup | grep haproxy
  ```

- **云供应商提供的负载均衡服务**

### Master初始化

```shell
#1、下载master节点需要的镜像【选做】
#!/bin/bash
images=(
	kube-apiserver:v1.17.3
    kube-proxy:v1.17.3
	kube-controller-manager:v1.17.3
	kube-scheduler:v1.17.3
	coredns:1.6.5
	etcd:3.4.3-0
    pause:3.1
)
for imageName in ${images[@]} ; do
    docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName
done
#2、初始化master节点，后面的步骤只需要在一个master节点上操作，其他master的初始化只需要执行　kubeadm join 即可初始化。
echo "127.0.0.1    k8s.apiserver" >> /etc/hosts
cat <<EOF > ./kubeadm-config.yaml
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: v1.17.3
imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
controlPlaneEndpoint: "k8s.apiserver:6443"
networking:
  serviceSubnet: "10.96.0.0/16"
  podSubnet: "10.244.0.0/16"
  dnsDomain: "cluster.local"
EOF
kubeadm init --config=kubeadm-config.yaml --upload-certs

# 这步骤可能遇到两个错误
[preflight] Running pre-flight checks
	[WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables contents are not set to 1
# 解决方法：
# 第一个警告：在/etc/docker/daemon.json中添加
# "exec-opts": ["native.cgroupdriver=systemd"]
# "exec-opts": ["native.cgroupdriver=cgroupfs"]	#改为这个,否则可能pod启动不起来
# 第二个错误：添加/etc/sysctl.d/kubernetes.conf文件，如果已经存在则追加内容
cat > /etc/sysctl.d/kubernetes.conf <<EOF
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.ipv4.ip_forward=1
vm.swappiness=0
vm.overcommit_memory=1
vm.panic_on_oom=0
fs.inotify.max_user_watches=89100
EOF
sysctl -p /etc/sysctl.d/kubernetes.conf
```

初始化成功后提示

```shell
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of the control-plane node running the following command on each as root:

  kubeadm join k8s.apiserver:6443 --token 71nzyi.loperqt8lesu68mu \
    --discovery-token-ca-cert-hash sha256:ccba8cab85eb9421f827c76a3cf0e6fb0c3fee80631410147d5265f2c8ee530f \
    --control-plane --certificate-key 9e83d156a9504458481b8fb75df88865e6538740c9cc67f961e2de82e1ea7325

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join k8s.apiserver:6443 --token 71nzyi.loperqt8lesu68mu \
    --discovery-token-ca-cert-hash sha256:ccba8cab85eb9421f827c76a3cf0e6fb0c3fee80631410147d5265f2c8ee530f 
```

初始化完成后按提示设置

```shell
# 设置配置
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
# 使用kubectl时可能报错：The connection to the server localhost:8080 was refused - did you specify the right host or port?
echo "export KUBECONFIG=/etc/kubernetes/admin.conf" >> ~/.bash_profile
source ~/.bash_profile
# 为集群设置pod网络
# 先写yaml配置文件，比如命名 k8snet-calico.yaml，参考https://kubernetes.io/docs/concepts/cluster-administration/addons/
# 这里选择CALICO网络插件
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
# 查看master状态是否就绪（即Running状态），就绪后继续后面操作
watch kubectl get pod -n kube-system -o wide
kubectl logs calico-node-nf87z	#查看某个组件容器状态（k8s插件是以docker容器方式部署的）
kubectl get nodes	#查看集群中节点
```

另两个Master节点的初始化

```shell
echo "192.168.2.190 	k8s.apiserver" >> /etc/hosts	#!!! 这里执行后文件中显示空格被删除了，还是手动进去修改比较好
# 注意，部署haproxy时因为是在k8s的master宿主机上部署的，不能占用6443,而是选用的16443绑定的6443,所以下面命令中也要改成这个端口。
kubeadm join k8s.apiserver:16443 --token 71nzyi.loperqt8lesu68mu \
    --discovery-token-ca-cert-hash sha256:ccba8cab85eb9421f827c76a3cf0e6fb0c3fee80631410147d5265f2c8ee530f \
    --control-plane --certificate-key 9e83d156a9504458481b8fb75df88865e6538740c9cc67f961e2de82e1ea7325
```

成功后提示：

```txt
This node has joined the cluster and a new control plane instance was created:

* Certificate signing request was sent to apiserver and approval was received.
* The Kubelet was informed of the new secure connection details.
* Control plane (master) label and taint were applied to the new node.
* The Kubernetes control plane instances scaled up.
* A new etcd member was added to the local/stacked etcd cluster.

To start administering your cluster from this node, you need to run the following as a regular user:

	mkdir -p $HOME/.kube
	sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
	sudo chown $(id -u):$(id -g) $HOME/.kube/config

Run 'kubectl get nodes' to see this node join the cluster.
```

查看三台Master节点状态

```shell
kubectl get node
# NAME     STATUS   ROLES    AGE     VERSION
# k8s-m1   Ready    master   37m     v1.17.3
# k8s-m2   Ready    master   7m56s   v1.17.3
# k8s-m3   Ready    master   2m18s   v1.17.3
```

## Node节点

添加k8s node节点到master

```shell
# 首先将Master虚拟IP加入Node节点的hosts文件
echo "192.168.2.190 	k8s.apiserver" >> /etc/hosts 

kubeadm join k8s.apiserver:16443 --token 71nzyi.loperqt8lesu68mu \
    --discovery-token-ca-cert-hash sha256:ccba8cab85eb9421f827c76a3cf0e6fb0c3fee80631410147d5265f2c8ee530f

# 为了能在Node节点中使用kubectl等命令最好执行下面命令
mkdir -p $HOME/.kube
# 然后将master /etc/kubernetes/admin.conf 内容拷贝到$HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

echo "export KUBECONFIG=~/.kube/config" >> ~/.bash_profile
source ~/.bash_profile
```

```shell
[root@k8s-m1 script]# kubectl get node -o wide
NAME     STATUS   ROLES    AGE     VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION                CONTAINER-RUNTIME
k8s-m1   Ready    master   123m    v1.17.3   192.168.2.191   <none>        CentOS Linux 7 (Core)   3.10.0-1127.13.1.el7.x86_64   docker://19.3.8
k8s-m2   Ready    master   93m     v1.17.3   192.168.2.192   <none>        CentOS Linux 7 (Core)   3.10.0-1127.13.1.el7.x86_64   docker://19.3.8
k8s-m3   Ready    master   87m     v1.17.3   192.168.2.193   <none>        CentOS Linux 7 (Core)   3.10.0-1127.13.1.el7.x86_64   docker://19.3.8
k8s-n1   Ready    <none>   16m     v1.17.3   192.168.2.194   <none>        CentOS Linux 7 (Core)   3.10.0-1127.13.1.el7.x86_64   docker://19.3.8
k8s-n2   Ready    <none>   7m49s   v1.17.3   192.168.2.195   <none>        CentOS Linux 7 (Core)   3.10.0-1127.13.1.el7.x86_64   docker://19.3.8
k8s-n3   Ready    <none>   108s    v1.17.3   192.168.2.196   <none>        CentOS Linux 7 (Core)   3.10.0-1127.13.1.el7.x86_64   docker://19.3.8
```

删除节点

```shell
#Master节点执行
kubectl drain <node name> --delete-local-data --force --ignore-daemonsets
kubectl delete node <node name>
#Node节点执行
kubeadm reset
```

## 其他组件

### Ingress网络

如果使用KubeSphere部署，可以使用相关配置项。

原生方式的话就是写yaml文件然后应用。

### Metrics-Server

### Provision-Storageclass

```shell
#1、所有节点安装 
yum install -y nfs-utils

#2、m1作为nfs-server执行如下配置，　如果要实现高可用通过在其他master节点挂载的方式实现
echo "/nfs/data	 *(insecure,rw,sync,no_root_squash)" > /etc/exports
mkdir -p /nfs/data

systemctl enable rpcbind
systemctl enable nfs-server
systemctl start rpcbind
systemctl start nfs-server
exportfs -r
#检查效果
exportfs

#3、设置动态provisioner
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-provisioner
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
   name: nfs-provisioner-runner
rules:
   -  apiGroups: [""]
      resources: ["persistentvolumes"]
      verbs: ["get", "list", "watch", "create", "delete"]
   -  apiGroups: [""]
      resources: ["persistentvolumeclaims"]
      verbs: ["get", "list", "watch", "update"]
   -  apiGroups: ["storage.k8s.io"]
      resources: ["storageclasses"]
      verbs: ["get", "list", "watch"]
   -  apiGroups: [""]
      resources: ["events"]
      verbs: ["watch", "create", "update", "patch"]
   -  apiGroups: [""]
      resources: ["services", "endpoints"]
      verbs: ["get","create","list", "watch","update"]
   -  apiGroups: ["extensions"]
      resources: ["podsecuritypolicies"]
      resourceNames: ["nfs-provisioner"]
      verbs: ["use"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-provisioner
    namespace: default
roleRef:
  kind: ClusterRole
  name: nfs-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
---
#vi nfs-deployment.yaml；创建nfs-client的授权
kind: Deployment
apiVersion: apps/v1
metadata:
   name: nfs-client-provisioner
spec:
   replicas: 1
   strategy:
     type: Recreate
   selector:
     matchLabels:
        app: nfs-client-provisioner
   template:
      metadata:
         labels:
            app: nfs-client-provisioner
      spec:
         serviceAccount: nfs-provisioner
         containers:
            -  name: nfs-client-provisioner
               image: quay.io/external_storage/nfs-client-provisioner:latest
               volumeMounts:
                 -  name: nfs-client-root
                    mountPath:  /persistentvolumes
               env:
                 -  name: PROVISIONER_NAME #供应者的名字
                    value: storage.pri/nfs #名字虽然可以随便起，以后引用要一致
                 -  name: NFS_SERVER
                    value: 192.168.2.191
                 -  name: NFS_PATH
                    value: /nfs/data
         volumes:
           - name: nfs-client-root
             nfs:
               server: 192.168.2.191 #nfs如果实现了高可用，这里是nfs的负载均衡地址
               path: /nfs/data
##这个镜像中volume的mountPath默认为/persistentvolumes，不能修改，否则运行时会报错
---
#创建storageclass
# vi storageclass-nfs.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: storage-nfs
provisioner: storage.pri/nfs
reclaimPolicy: Delete

#改变系统默认sc
kubectl patch storageclass storage-nfs -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
```

